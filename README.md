# CPS4951-Capstone
Capstone code and database
## ğŸ¬ IMDb Sentiment Analysis
## 1. Project Overview
This repository implements sentiment classification on the IMDb dataset and compares following model:
- **LDA + Logistic Regression**
- **BiLSTM**
- **BERT**
- **LDA-BERT**
- **BERT-BiLSTM**<br>
Each model is trained and evaluated across multiple fixed random seeds to ensure experimental reproducibility.
After training, the framework automatically computes and records several key performance metrics, including Accuracy, Macro-F1, AUC, and Running Time.
To assess statistical significance, paired t-tests are conducted between models.
In addition, detailed error analysis is performed to identify false positives and false negatives, followed by the generation of confusion matrices and ROC curves for visual comparison.

Overall, this project aims to explore the performance, efficiency, and reliability trade-offs among classical, deep, and hybrid NLP models under a controlled experimental setting.

## ğŸ§± Project Structure 
### ğŸ“‚ Before Running 
```bash
.
â”œâ”€â”€ IMDB Dataset.zip          # Raw IMDb dataset 
â”œâ”€â”€ bert-base-uncased.zip     # Offline BERT model snapshot
â”œâ”€â”€ code.ipynb                # Main notebook for experiments 
â”œâ”€â”€ environment.yml           # Conda environment definition
â”œâ”€â”€ requirements.txt          # pip dependency list
â””â”€â”€ README.md                 # Project documentation 
```

## âš™ï¸ Environment Setup

There are two ways to set up the environment for this project:

1. Using **pip** with `requirements.txt`
2. Using **Conda** with the provided `environment.yml`

It is recommended to install dependencies using **pip** for simplicity and speed.
If any environment conflicts occur (e.g., CUDA or PyTorch version issues), you may alternatively use **Conda**.

### ğŸ”¹ Option A â€” Using pip
```bash
# Install all dependencies
pip install -r requirements.txt
```

### ğŸ”¹ Option B â€” Using Conda
```bash
# Create a new environment
conda env create -f environment.yml
# Activate the environment
conda activate imdb-sentiment
```

### ğŸ”¸ Prepare Offline Files

```bash
# Unzip IMDb dataset
unzip "IMDB Dataset.zip" -d data/

# Unzip BERT base model snapshot
unzip "bert-base-uncased.zip"
```
## ğŸš€ Running the Project

After setting up the environment and unzipping the dataset and model files,  
you can run all experiments directly from the Jupyter Notebook.

### ğŸ”¹ Step 1 â€” Launch Jupyter Notebook
```bash
jupyter notebook
```
### ğŸ”¹ Step 2 â€” Open the Notebook
```bash
# Open the file:
code.ipynb
```
### ğŸ”¹ Step 3 â€” Run All Cells
Run all cells in sequence.
The notebook will automatically:
  load and preprocess the IMDb dataset,
  train and evaluate five models (LDA+LR, BiLSTM, BERT, LDA-BERT, BERT-BiLSTM),
  repeat experiments across five random seeds [42, 77, 4096, 9898, 2025],
  compute metrics including Accuracy, Macro-F1, AUC, and Running Time,
  perform paired t-tests and error analysis,
  generate confusion matrices and ROC curves
### ğŸ”¹ Step 4 â€” Check the Outputs
All results are automatically saved under the results/ directory:
```bash
results/
â”œâ”€â”€<model><seeds>_learn.csv
â”œâ”€â”€<model>_run.csv
â”œâ”€â”€ preds/          # per-seed prediction files
â”œâ”€â”€ figs/           # ROC & confusion matrix plots
â”œâ”€â”€ tables/         # summary tables (mean/std, t-tests)
â”œâ”€â”€ final_preds/    # aggregated results
â””â”€â”€ error_samples/  # misclassified samples for analysis
```
## ğŸ“Š Example Outputs
### ğŸ”¹ Step 1 â€” Check the base lib
Prepore for load data<br>
![check](Figure/cell1.jpg)
### ğŸ”¹ Step 2 â€” load data and split
50000->35000/7500/7500<br>
![split](Figure/cell2.jpg)
### ğŸ”¹ Step 3 â€” Clean data and save split dataset
Remove HTML tags, URLs, punctuation, digits.
Convert to lowercase and normalize whitespace.
Add a backup column text_raw to preserve original text.
Filter out empty or whitespace-only samples.
![clean](Figure/cell3.jpg)
### ğŸ”¹ Step 4 â€” LDA
![clean](Figure/LDA.jpg)

### ğŸ”¹ Step 5 â€” BiLSTM
![clean](Figure/BiLSTM.jpg)

### ğŸ”¹ Step 6 â€” BERT
![clean](Figure/BERT.jpg)

### ğŸ”¹ Step 7 â€” LDA-BERT
![clean](Figure/LDA-BERT.jpg)

### ğŸ”¹ Step 8 â€” BERT-BiLSTM
![clean](Figure/BERT-BiLSTM.jpg)

### ğŸ”¹ Step 9 â€” Output summary.csv
![clean](Figure/summary.jpg)

### ğŸ”¹ Step 10 â€” Error simple, ROC & Confusion Martix
![clean](Figure/E,R,C.jpg)

### ğŸ”¹ Step 11 â€” pring figure from result 
![clean](Figure/T-test.jpg)

## ğŸ Results Summary

Deep learning models significantly outperformed the traditional LDA + Logistic Regression baseline on the IMDb sentiment analysis task.  
Among all models, **BERT** achieved the highest performance (Accuracy = 0.9272, Macro-F1 = 0.9272, AUC = 0.9784).  
Hybrid architectures such as **LDA-BERT** and **BERT-BiLSTM** did not surpass single BERT, likely due to feature redundancy and weak fusion design.  
Overall, BERT proved to be the most effective and efficient approach, while future work can explore improved fusion strategies or more complex datasets.


## âš–ï¸ License
This project is released under the MIT License.  
Feel free to use and modify the code with proper attribution.









